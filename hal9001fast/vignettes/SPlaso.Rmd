---
title: "SparseGroup"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r}
library(R6)
devtools::document()
```


```{r}
formula_hal()


```


```{r}
library(sl3)
n=100
k=5

folds = compute_subsemble_fold(n,k)
strata_ids = attr(folds, "strata_ids")
strata_ids
lrnr = make_learner(Lrnr_subsemble, lrnr = make_learner(Lrnr_glm), strata_ids = strata_ids)

```


```{r}
library(sl3)
library(delayed)
library(data.table)
 stk = make_learner(sl3::Stack, make_learner(Lrnr_glm), make_learner(Lrnr_glmnet))
lrnr = make_learner(Lrnr_subsemble, lrnr =stk, strata_ids = strata_ids)
#lrnr = lrnr$train(task)
#tsk = lrnr$chain(task)
#tsk$X
# lrnr = lrnr$train(task)
# lrnr$learner_fits$Lrnr_glm_TRUE_strata_1$base_predict()
# lrnr$predict(task)
lrnr_subsemble <- make_learner(Pipeline, lrnr, Lrnr_subsemble_metalearner$new(strata_ids = strata_ids))
lrn = lrnr_subsemble$train(task)

```


```{r, include = F}
library(R6)
devtools::document()
```



```{r}

t=proc.time()

f=run_descent_gaussian(rbind(x),c(y),as.vector(beta-.01), max_iter = 1000, verbose=T)
proc.time()-t
```


```{r}
plot(y,x%*%(beta+.01))
plot(y,x%*%f)
```


```{r}
t = proc.time()
mat =  replicate(10,calc_canon_grad(x,Y,beta))
t1 = proc.time()
t1 - t
for(i in 1:10){
link = y - x %*% beta
}
t2 = proc.time()
t2 - t1
```


```{r}

link = y - x %*% beta
x_summed =  Matrix::crossprod(x, link)
grad = x_summed * beta
t1 = proc.time()
betas = as.vector(beta+1)
for(i in 1:100){
link = y - x %*% beta
 
}
 proc.time() - t1
```

```{r}
beta + 1

```
```{r}
library(R6)
devtools::document()

```




```{r}

lambda = c(1,0.5)
f=make_learner_subsemble_hal(make_learner(Lrnr_hal9001fast, max_degree=1,cv_select=F, lambda = lambda, verbose=F), task,k=4, num_preds = length(lambda))
f = delayed_learner_train(f[[1]], f[[2]])

sched <- Scheduler$new(f, FutureJob, nworkers = 1, verbose = F)
f <- sched$compute()
f$predict()
```


```{r}
a =f$fit_object$learner_fits[[2]]$fit_object$learner_fits[[1]]$fit_object$beta
c = f$fit_object$learner_fits[[2]]$fit_object$learner_fits[[1]]$fit_object$hal_fits[[2]]$coefs
sum(abs(a))
sum(abs(c))
```

```{r}
library(R6)

devtools::document()
library(R6)
n=1000
X = as.matrix(cbind(replicate(20, runif(n,1,2))))
X = data.frame(X)
trueY = 2*(rowSums(X[,1:10,drop=F]))^2 + 2*(rowSums(X[,11:20,drop=F]^2))
Y = trueY  + rnorm(n,0,1.2)

data = data.frame(cbind(X,Y))
colnames(data) = c(colnames(X), "y")
task = sl3_hal_Task$new(data, covariates = colnames(X), outcome = "y")
```

```{r}
lrnr <- make_learner(Lrnr_hal9001fast, max_degree = 3, cv_select = F,bins=c(100,50,10),  cor_pval=0.05, dcor_pval = NULL, screen_basis_main_terms=F, screen_basis_interactions=F, smoothness_orders=1, max_total_basis = 50000)
tmp = lrnr$train(task)
lambda = tmp$fit_object$fit$lambda_star
lrnr <- make_learner(Lrnr_hal9001fast, max_degree = 2, cv_select = F,bins= c(500,50,5),  cor_pval=0.15,screen_basis_main_terms=T, screen_basis_interactions=F, lambda = lambda, smoothness_orders = 1)
#lrnr <- lrnr$train(task)
cv_hal = make_learner(Pipeline, Lrnr_cv$new(lrnr), Lrnr_cv_selector$new())
```















```{r, include=F}
library(delayed)
library(sl3)
library(origami)

cv_hal = delayed_learner_train(cv_hal, task)
sched <- Scheduler$new(cv_hal, FutureJob, nworkers = 4, verbose = T)
cv_fit <- sched$compute()
cv_fit
```

```{r}

preds= cv_fit$predict()
mean((preds -trueY)^2)
mean((preds - Y )^2)
plot(preds,trueY )
plot(preds,Y)
plot(Y,trueY)
```




```{r}
s=energy::dcorT.test(Y,Y)
s$p.value
```

```{r}
fit = fit_hal(X,Y,max_degree = 1, screen_basis_main_terms = T,  num_bins=min(n/10,500), smoothness_orders = 0)


```
```{r}
length(fit$basis_list)
plot(predict(fit, new_data = testX),testY)


mean((predict(fit, new_data = testX)-testY)^2)



```



```{r}



init_reduce = 0.25
red_seq =  c(0.2,0.15,0.1,0.05,0.01)
for(reduce in red_seq){
  print(sumry$mins)
  
  sumry$mins = c(sumry$mins, min(initfit$cvm))
  
reduced_basis_map <- hal9001:::make_reduced_basis_map(x, reduce)
exclude = setdiff(1:ncol(x), reduced_basis_map)

initfit = cv.glmnet(x, Y, exclude = exclude, foldid = foldid)
}
sumry$mins
```

```{r}

plot(red_seq,sumry$mins)

```

```{r}
next_fit = function(prevfit, x, Y, reduce, i=NULL){
  if("glmnetfit" %in% class(prevfit)){
    warm = list(a0 = prevfit$a0, beta = prevfit$beta)
    lambda = prevfit$lambda
  }
  else{
     print(i)

    
  
    warm = list(a0 = prevfit$a0[i], beta = as.vector(prevfit$beta[,i]))
    lambda = prevfit$lambda[i]

  }
  reduced_basis_map <- hal9001:::make_reduced_basis_map(x, reduce)
  exclude = setdiff(1:ncol(x), reduced_basis_map)
  
  fit_warm = glmnet:::glmnet.fit(x,Y, weights = rep(1, length(Y)), exclude =exclude, warm = warm, lambda = lambda )
  
}

```

```{r}

lambda_path = initfit$lambda
fits = list()
prev =  glmnet:::glmnet.fit(x,Y, weights = rep(1, length(Y)), exclude =exclude, lambda = lambda_path[1] )
for(lambda in lambda_path){
  print(lambda)
  cur = glmnet:::glmnet.fit(x,Y, weights = rep(1, length(Y)), exclude =exclude, warm = list(a0=prev$a0, beta = as.vector(prev$beta)), lambda = lambda )
  fits = c(fits, list(cur))
  prev = cur
}
```

```{r}
reduce = 0.1
reduce_seq = seq(reduce, 0.001, by=-0.005)
fits_all = list()
fits_all[[1]] = fits


doOneStep = function(i){
  reduce = reduce_seq[i]
   fits = fits_all[[i-1]]
   newfits = list()
   print(reduce)
   for(f in fits){
     print(f$lambda)
     if(is.null(f$lambda))
     newfits = c(newfits, list(next_fit(f,x,Y,reduce)))
   }
   fits_all[[i]] <<- newfits
}

doOneStep(3)

```

```{r}


```



```{r}
f = fits[[50]]
a=f
for(reduce in reduce_seq){
  for(f in fits[1:3]){
    
  print(sum(f$beta != a$beta & (f$beta==0 | a$beta==0)))
a = (next_fit(a,x,Y,reduce))
fits_red = c(fits_red,list(a))
}

}
f=glmnet:::glmnet.fit(x=x,y=Y, weights = rep(1,n), lambda = 0.5)

```

