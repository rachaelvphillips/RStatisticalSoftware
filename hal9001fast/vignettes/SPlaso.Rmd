---
title: "SparseGroup"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r}
devtools::document()

```


```{r}
is.vector(3)

```



```{r}
library(sl3)
n=1000
X = as.matrix(cbind(replicate(30, runif(n,1,2))))
X = data.frame(X)
Y = 2*rowSums((X[,1:20,drop=F]))^3 + rnorm(n,0,2)

data = data.frame(cbind(X,Y))
colnames(data) = c(colnames(X), "y")
task = make_sl3_Task(data, covariates = colnames(X), outcome = "y")

lrnr <- make_learner(Lrnr_hal9001fast, max_degree =3, cv_select = T,bins=100,  cor_pval=0.15,screen_basis_main_terms=T, screen_basis_interactions=F)
lrnr <- lrnr$train(task)

```





```{r}

X = as.matrix(cbind(replicate(10, runif(n))))
X = data.frame(X)
trueY =  2*rowSums((X[,1:3,drop=F]))^2
Y = trueY + rnorm(n,0,1)

data = data.frame(cbind(X,Y))
colnames(data) = c(colnames(X), "y")
task = make_sl3_Task(data, covariates = colnames(X), outcome = "y")
plot(2*rowSums(sin(10*X[,1:3,drop=F]))^2,lrnr$predict(task))
plot(2*rowSums(sin(10*X[,1:3,drop=F]))^2,Y)
plot(trueY,lrnr$predict(task))

mean(( 2*rowSums(sin(10*X[,1:3,drop=F]))^2 - Y)^2)
mean(( lrnr$predict(task) - Y)^2)
mean(( lrnr$predict(task) - trueY)^2)
```







```{r}
lrnr$fit_object$fit$basis_list
```

```{r}
s=energy::dcorT.test(Y,Y)
s$p.value
```

```{r}
fit = fit_hal(X,Y,max_degree = 1, screen_basis_main_terms = T,  num_bins=min(n/10,500), smoothness_orders = 0)


```
```{r}
length(fit$basis_list)
plot(predict(fit, new_data = testX),testY)


mean((predict(fit, new_data = testX)-testY)^2)



```



```{r}



init_reduce = 0.25
red_seq =  c(0.2,0.15,0.1,0.05,0.01)
for(reduce in red_seq){
  print(sumry$mins)
  
  sumry$mins = c(sumry$mins, min(initfit$cvm))
  
reduced_basis_map <- hal9001:::make_reduced_basis_map(x, reduce)
exclude = setdiff(1:ncol(x), reduced_basis_map)

initfit = cv.glmnet(x, Y, exclude = exclude, foldid = foldid)
}
sumry$mins
```

```{r}

plot(red_seq,sumry$mins)

```

```{r}
next_fit = function(prevfit, x, Y, reduce, i=NULL){
  if("glmnetfit" %in% class(prevfit)){
    warm = list(a0 = prevfit$a0, beta = prevfit$beta)
    lambda = prevfit$lambda
  }
  else{
     print(i)

    
  
    warm = list(a0 = prevfit$a0[i], beta = as.vector(prevfit$beta[,i]))
    lambda = prevfit$lambda[i]

  }
  reduced_basis_map <- hal9001:::make_reduced_basis_map(x, reduce)
  exclude = setdiff(1:ncol(x), reduced_basis_map)
  
  fit_warm = glmnet:::glmnet.fit(x,Y, weights = rep(1, length(Y)), exclude =exclude, warm = warm, lambda = lambda )
  
}

```

```{r}

lambda_path = initfit$lambda
fits = list()
prev =  glmnet:::glmnet.fit(x,Y, weights = rep(1, length(Y)), exclude =exclude, lambda = lambda_path[1] )
for(lambda in lambda_path){
  print(lambda)
  cur = glmnet:::glmnet.fit(x,Y, weights = rep(1, length(Y)), exclude =exclude, warm = list(a0=prev$a0, beta = as.vector(prev$beta)), lambda = lambda )
  fits = c(fits, list(cur))
  prev = cur
}
```

```{r}
reduce = 0.1
reduce_seq = seq(reduce, 0.001, by=-0.005)
fits_all = list()
fits_all[[1]] = fits


doOneStep = function(i){
  reduce = reduce_seq[i]
   fits = fits_all[[i-1]]
   newfits = list()
   print(reduce)
   for(f in fits){
     print(f$lambda)
     if(is.null(f$lambda))
     newfits = c(newfits, list(next_fit(f,x,Y,reduce)))
   }
   fits_all[[i]] <<- newfits
}

doOneStep(3)

```

```{r}


```



```{r}
f = fits[[50]]
a=f
for(reduce in reduce_seq){
  for(f in fits[1:3]){
    
  print(sum(f$beta != a$beta & (f$beta==0 | a$beta==0)))
a = (next_fit(a,x,Y,reduce))
fits_red = c(fits_red,list(a))
}

}
f=glmnet:::glmnet.fit(x=x,y=Y, weights = rep(1,n), lambda = 0.5)

```

