---
title: "Day57bindSpike"
output:
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = F)
```

```{r}
library(kyotil)
dat.mock=read.csv("https://raw.githubusercontent.com/youyifong/CovidCorrSAP/master/data_processing/COVID_VEtrial_practicedata_longerterm.csv?token=ACHPI424NLY5HHKZKDGGVKC7TMGAU")

names(dat.mock)[1]="Ptid"
times=c("B","Day57")
assays=c("bindSpike","bindRBD","pseudoneut","liveneut")
markers=c(outer(times, assays, "%.%"))


dat.mock$age.geq.65=as.integer(dat.mock$Age>=65)


###########################################################################
# stratum and weights

# Bstratum, 1-8, defines the 8 baseline strata within trt/serostatus
dat.mock$Bstratum=1+strtoi(with(dat.mock, paste0(MinorityInd, HighRiskInd, as.integer(Age>=65))), base = 2)
# tps stratum, 1-32, used in tps regression
dat.mock$tps.stratum = 1+strtoi(with(dat.mock, paste0(Trt, Bserostatus, MinorityInd, HighRiskInd, as.integer(Age>=65))), base = 2)
# Wstratum, 1-33. Differs from tps stratum in that case is a separate stratum. Used to compute sampling weights
dat.mock$Wstratum = ifelse(dat.mock$EventIndLongterm==1, 1+max(dat.mock$tps.stratum), dat.mock$tps.stratum) 

dat.mock$hasAllMarkers = complete.cases(dat.mock[markers])

with(dat.mock, table(Wstratum, hasAllMarkers, TwophasesampIndLongterm))

# compute inverse sampling prob weight, assuming that we will impute missing values for all in ph2
tmp=with(dat.mock, table(Wstratum, TwophasesampIndLongterm))
weights=rowSums(tmp)/tmp[,2]
dat.mock$wt=weights[dat.mock$Wstratum]

with(dat.mock, table(TwophasesampIndLongterm, Perprotocol))

dat.mock.ph2=subset(dat.mock, TwophasesampIndLongterm==1 & Perprotocol)
with(dat.mock.ph2, table(Wstratum, hasAllMarkers))
 

###########################################################################
# impute missing immune biomarkers in ph2
#     impute vaccine and placebo separately
#     use all assays
#     use baseline and D57, but not Delta

library(mice)
set.seed(1)
n.imp=10

for (.trt in 0:1) {
    imp=mice(dat.mock.ph2[dat.mock.ph2$Trt==.trt, markers],m=n.imp)
    # use the first imputation by default
    dat.mock.ph2[dat.mock.ph2$Trt==.trt, markers]=mice::complete(imp, action=1)
    # keep n.imp imputations
    for (i in 1:n.imp) dat.mock.ph2[dat.mock.ph2$Trt==.trt, markers%.%".imp"%.%i]=mice::complete(imp, action=i)
}
stopifnot(all(table(dat.mock.ph2$Wstratum, complete.cases(dat.mock.ph2[markers]))))

# populate dat.mock markers with the imputed values
dat.mock[markers]=dat.mock.ph2[markers][match(dat.mock$Ptid, dat.mock.ph2$Ptid),]
for (i in 1:n.imp) dat.mock[markers%.%".imp"%.%i]=dat.mock.ph2[markers%.%".imp"%.%i][match(dat.mock$Ptid, dat.mock.ph2$Ptid),]


###########################################################################
# define delta for both dat.mock and dat.mock.ph2

dat.mock    ["Delta"%.%assays]=dat.mock    ["Day57"%.%assays]-dat.mock    ["B"%.%assays]
dat.mock.ph2["Delta"%.%assays]=dat.mock.ph2["Day57"%.%assays]-dat.mock.ph2["B"%.%assays]
for (i in 1:n.imp) dat.mock    ["Delta"%.%assays%.%".imp"%.%i]=dat.mock    ["Day57"%.%assays%.%".imp"%.%i]-dat.mock    ["B"%.%assays%.%".imp"%.%i]
for (i in 1:n.imp) dat.mock.ph2["Delta"%.%assays%.%".imp"%.%i]=dat.mock.ph2["Day57"%.%assays%.%".imp"%.%i]-dat.mock.ph2["B"%.%assays%.%".imp"%.%i]


###########################################################################



```


```{r}
dat.long
dat.mock
```
# 40
```{r, include = T}
data <- dat.mock
dim(data)

keep <- data$Trt==1 & data$Bserostatus==0 & data$Perprotocol==1
baseline <- c("MinorityInd", "HighRiskInd", "Age", "BRiskScore")
marker <- "Day57bindSpike"

outcome <- "EventIndLongterm"
data <- data[keep, c(outcome, marker, baseline)]
data <- na.omit(data)
sum(is.na(data))

#lrnr <- Lrnr_pooled_hazards$new(Lrnr_xgboost$new())
# To use machine learning
library(earth)

for(thresh in c(quantile(data[[marker]], c(0.1, 0.3, 0.5, 0.7, 0.8 ,0.9, 0.95)))){
thresholds <- c(thresh)
print(thresholds)

lrnr_Y <- Lrnr_glmnet$new()
lrnr_A <-  Lrnr_glmnet$new()



tmle_spec <- tmle3_Spec_Threshold$new(method = "Psi_W", threshold_function = thresholds, cv = F)

learner_list <- list("A" = lrnr_A, "Y" =lrnr_Y)
node_list <- list("W" = baseline, "A" = marker, "Y" = outcome)


tmle_task <- tmle_spec$make_tmle_task(data, node_list)
print("donef")
initial_likelihood <- tmle_spec$make_initial_likelihood(tmle_task, learner_list)
print("done")
updater <- tmle_spec$make_updater(maxit = 100, verbose = T)
targeted_likelihood <- tmle_spec$make_targeted_likelihood(initial_likelihood, updater)
print("k")
print(length(targeted_likelihood$get_likelihood(tmle_task, "Y")))
print("k")
print(length(targeted_likelihood$get_likelihood(tmle_task, "A")))
print("k")
tmle_params <- tmle_spec$make_params(tmle_task, targeted_likelihood)
print("done")
(targeted_likelihood$updater$update_step(targeted_likelihood, tmle_task))
print("done")
#Donvan estimator

weights <-1
get_estimates <- function(data, marker_var, outcome_var, thresholds, weights = rep(1, nrow(data))) {
  weights <- weights/sum(weights)
  data <- as.data.table(data)
  IC_list <- list()
  est_list <- list()
  cdf_list <- c()
  for(thresh in thresholds) {
    meets_thresh <- as.numeric(data[[marker_var]] >= thresh) 
    cdf <- weighted.mean(meets_thresh, weights)
    cdf_list <- c(cdf_list, cdf)
    EY <- weighted.mean(data[[outcome_var]] * meets_thresh, weights) / cdf
    thresh <- as.character(thresh)
    est_list[[thresh]] <- EY
    IC_list[[thresh]] <- meets_thresh/cdf * (data[[outcome_var]] - EY)
    
  }
  return(list(est = unlist(est_list),
             IC = do.call(cbind, IC_list), cdf = 1-cdf_list ))
}
out <- get_estimates(data, marker, outcome, thresholds)
est <- out$est
IC <- out$IC * weights
radius <- 1.96 * sqrt(resample::colVars(IC))/sqrt(nrow(data))
lower <- est - radius
upper <- est + radius
donovan_est <- data.table(lower, est, upper )

estimates <- tmle_params[[1]]$estimates(tmle_task)
# This should be very small
# Estimates of Psi
estimates$psi

# Estimates and confidence bounds
sumry <- summary_from_estimates(tmle_task, list(estimates))[,-c(1,2,3)]
lower <- sumry$lower
upper <- sumry$upper
est <- sumry$tmle_est
cutoffs <- thresholds

cutoffs <- round(targeted_likelihood$factor_list$Y$learner$cutoffs,3)

plot_data <- data.frame(est = est, lower = lower, upper = upper, cutoffs = round(targeted_likelihood$factor_list$Y$learner$cutoffs,3))
plot_data2 <- donovan_est
plot_data2$cutoffs <- cutoffs
plot_data$grp <- "TMLE"
plot_data2$grp <- "Donovan"
plot_data <- rbind(plot_data, plot_data2)
library(ggplot2)

 g1 <- ggplot2::ggplot(data =  plot_data, aes_string("cutoffs", "est", group = "grp", col="grp", fill="grp"))+ scale_x_continuous(breaks = plot_data$cutoffs) +
                              geom_point(aes_string(x = "cutoffs", y = "est",group = "grp", col="grp", fill="grp"), legend=  F,   colour=alpha('red')) +
                              geom_point(aes_string(x = "cutoffs", y ="lower",group = "grp", col="grp", fill="grp"), legend=  F, alpha = 0) +
                              geom_point(aes_string(x = "cutoffs", y = "upper",group = "grp", col="grp", fill="grp"), legend=  F, alpha = 0) +
                              geom_smooth(se = F)+
                              geom_ribbon(aes(ymin = lower, ymax = upper), alpha= 0.2, color = NA) +
                              scale_x_continuous(breaks = plot_data$cutoffs)
 
print(plot_data)

print(g1)
}
```

# 30
```{r, include = F}
data <- dat.mock.vacc.seroneg.30
dim(data)

keep <- data$Trt==1 & data$Bserostatus==0 & data$Perprotocol==1
baseline <- c("MinorityInd", "HighRiskInd", "Age", "BRiskScore")


outcome <- "EventIndLongterm"
data <- data[keep, c(outcome, marker, baseline)]
data <- na.omit(data)
sum(is.na(data))

#lrnr <- Lrnr_pooled_hazards$new(Lrnr_xgboost$new())
# To use machine learning
library(earth)







tmle_spec <- tmle3_Spec_Threshold$new(method = "Psi_W", threshold_function = thresholds, cv = F)

learner_list <- list("A" = lrnr_A, "Y" =lrnr_Y)
node_list <- list("W" = baseline, "A" = marker, "Y" = outcome)


tmle_task <- tmle_spec$make_tmle_task(data, node_list)

initial_likelihood <- tmle_spec$make_initial_likelihood(tmle_task, learner_list)

updater <- tmle_spec$make_updater(maxit = 100, verbose = T)
targeted_likelihood <- tmle_spec$make_targeted_likelihood(initial_likelihood, updater)

tmle_params <- tmle_spec$make_params(tmle_task, targeted_likelihood)

(targeted_likelihood$updater$update_step(targeted_likelihood, tmle_task))

#Donvan estimator

weights <-1
get_estimates <- function(data, marker_var, outcome_var, thresholds, weights = rep(1, nrow(data))) {
  weights <- weights/sum(weights)
  data <- as.data.table(data)
  IC_list <- list()
  est_list <- list()
  cdf_list <- c()
  for(thresh in thresholds) {
    meets_thresh <- as.numeric(data[[marker_var]] >= thresh) 
    cdf <- weighted.mean(meets_thresh, weights)
    cdf_list <- c(cdf_list, cdf)
    EY <- weighted.mean(data[[outcome_var]] * meets_thresh, weights) / cdf
    thresh <- as.character(thresh)
    est_list[[thresh]] <- EY
    IC_list[[thresh]] <- meets_thresh/cdf * (data[[outcome_var]] - EY)
    
  }
  return(list(est = unlist(est_list),
             IC = do.call(cbind, IC_list), cdf = 1-cdf_list ))
}
out <- get_estimates(data, marker, outcome, thresholds)
est <- out$est
IC <- out$IC * weights
radius <- 1.96 * sqrt(resample::colVars(IC))/sqrt(nrow(data))
lower <- est - radius
upper <- est + radius
donovan_est <- data.table(lower, est, upper )

estimates <- tmle_params[[1]]$estimates(tmle_task)
# This should be very small
# Estimates of Psi
estimates$psi

# Estimates and confidence bounds
sumry <- summary_from_estimates(tmle_task, list(estimates))[,-c(1,2,3)]
lower <- sumry$lower
upper <- sumry$upper
est <- sumry$tmle_est
cutoffs <- thresholds
```

```{r, include = F}
cutoffs <- round(targeted_likelihood$factor_list$Y$learner$cutoffs,3)

plot_data <- data.frame(est = est, lower = lower, upper = upper, cutoffs = round(targeted_likelihood$factor_list$Y$learner$cutoffs,3))
plot_data2 <- donovan_est
plot_data2$cutoffs <- cutoffs
plot_data$grp <- "TMLE"
plot_data2$grp <- "Donovan"
plot_data <- rbind(plot_data, plot_data2)
library(ggplot2)

 g1 <- ggplot2::ggplot(data =  plot_data, aes_string("cutoffs", "est", group = "grp", col="grp", fill="grp"))+ scale_x_continuous(breaks = plot_data$cutoffs) +
                              geom_point(aes_string(x = "cutoffs", y = "est",group = "grp", col="grp", fill="grp"), legend=  F,   colour=alpha('red')) +
                              geom_point(aes_string(x = "cutoffs", y ="lower",group = "grp", col="grp", fill="grp"), legend=  F, alpha = 0) +
                              geom_point(aes_string(x = "cutoffs", y = "upper",group = "grp", col="grp", fill="grp"), legend=  F, alpha = 0) +
                              geom_smooth(se = F)+
                              geom_ribbon(aes(ymin = lower, ymax = upper), alpha= 0.2, color = NA) +
                              scale_x_continuous(breaks = plot_data$cutoffs)
 


```

```{r}
g1


```


# 25

```{r, include = F}
data <- dat.mock.vacc.seroneg.25
dim(data)

keep <- data$Trt==1 & data$Bserostatus==0 & data$Perprotocol==1
baseline <- c("MinorityInd", "HighRiskInd", "Age", "BRiskScore")


outcome <- "EventIndLongterm"
data <- data[keep, c(outcome, marker, baseline)]
data <- na.omit(data)
sum(is.na(data))

#lrnr <- Lrnr_pooled_hazards$new(Lrnr_xgboost$new())
# To use machine learning
library(earth)






tmle_spec <- tmle3_Spec_Threshold$new(method = "Psi_W", threshold_function = thresholds, cv = F)

learner_list <- list("A" = lrnr_A, "Y" =lrnr_Y)
node_list <- list("W" = baseline, "A" = marker, "Y" = outcome)


tmle_task <- tmle_spec$make_tmle_task(data, node_list)

initial_likelihood <- tmle_spec$make_initial_likelihood(tmle_task, learner_list)

updater <- tmle_spec$make_updater(maxit = 100, verbose = T)
targeted_likelihood <- tmle_spec$make_targeted_likelihood(initial_likelihood, updater)

tmle_params <- tmle_spec$make_params(tmle_task, targeted_likelihood)

(targeted_likelihood$updater$update_step(targeted_likelihood, tmle_task))

#Donvan estimator

weights <-1
get_estimates <- function(data, marker_var, outcome_var, thresholds, weights = rep(1, nrow(data))) {
  weights <- weights/sum(weights)
  data <- as.data.table(data)
  IC_list <- list()
  est_list <- list()
  cdf_list <- c()
  for(thresh in thresholds) {
    meets_thresh <- as.numeric(data[[marker_var]] >= thresh) 
    cdf <- weighted.mean(meets_thresh, weights)
    cdf_list <- c(cdf_list, cdf)
    EY <- weighted.mean(data[[outcome_var]] * meets_thresh, weights) / cdf
    thresh <- as.character(thresh)
    est_list[[thresh]] <- EY
    IC_list[[thresh]] <- meets_thresh/cdf * (data[[outcome_var]] - EY)
    
  }
  return(list(est = unlist(est_list),
             IC = do.call(cbind, IC_list), cdf = 1-cdf_list ))
}
out <- get_estimates(data, marker, outcome, thresholds)
est <- out$est
IC <- out$IC * weights
radius <- 1.96 * sqrt(resample::colVars(IC))/sqrt(nrow(data))
lower <- est - radius
upper <- est + radius
donovan_est <- data.table(lower, est, upper )

estimates <- tmle_params[[1]]$estimates(tmle_task)
# This should be very small
# Estimates of Psi
estimates$psi

# Estimates and confidence bounds
sumry <- summary_from_estimates(tmle_task, list(estimates))[,-c(1,2,3)]
lower <- sumry$lower
upper <- sumry$upper
est <- sumry$tmle_est
cutoffs <- thresholds
```

```{r, include = F}
cutoffs <- round(targeted_likelihood$factor_list$Y$learner$cutoffs,3)

plot_data <- data.frame(est = est, lower = lower, upper = upper, cutoffs = round(targeted_likelihood$factor_list$Y$learner$cutoffs,3))
plot_data2 <- donovan_est
plot_data2$cutoffs <- cutoffs
plot_data$grp <- "TMLE"
plot_data2$grp <- "Donovan"
plot_data <- rbind(plot_data, plot_data2)
library(ggplot2)

 g1 <- ggplot2::ggplot(data =  plot_data, aes_string("cutoffs", "est", group = "grp", col="grp", fill="grp"))+ scale_x_continuous(breaks = plot_data$cutoffs) +
                              geom_point(aes_string(x = "cutoffs", y = "est",group = "grp", col="grp", fill="grp"), legend=  F,   colour=alpha('red')) +
                              geom_point(aes_string(x = "cutoffs", y ="lower",group = "grp", col="grp", fill="grp"), legend=  F, alpha = 0) +
                              geom_point(aes_string(x = "cutoffs", y = "upper",group = "grp", col="grp", fill="grp"), legend=  F, alpha = 0) +
                              geom_smooth(se = F)+
                              geom_ribbon(aes(ymin = lower, ymax = upper), alpha= 0.2, color = NA) +
                              scale_x_continuous(breaks = plot_data$cutoffs)
 


```

```{r}
g1


```


# 20

```{r, include = F}
data <- dat.mock.vacc.seroneg.20
dim(data)

keep <- data$Trt==1 & data$Bserostatus==0 & data$Perprotocol==1
baseline <- c("MinorityInd", "HighRiskInd", "Age", "BRiskScore")


outcome <- "EventIndLongterm"
data <- data[keep, c(outcome, marker, baseline)]
data <- na.omit(data)
sum(is.na(data))

#lrnr <- Lrnr_pooled_hazards$new(Lrnr_xgboost$new())
# To use machine learning
library(earth)




tmle_spec <- tmle3_Spec_Threshold$new(method = "Psi_W", threshold_function = thresholds, cv = F)

learner_list <- list("A" = lrnr_A, "Y" =lrnr_Y)
node_list <- list("W" = baseline, "A" = marker, "Y" = outcome)


tmle_task <- tmle_spec$make_tmle_task(data, node_list)

initial_likelihood <- tmle_spec$make_initial_likelihood(tmle_task, learner_list)

updater <- tmle_spec$make_updater(maxit = 100, verbose = T)
targeted_likelihood <- tmle_spec$make_targeted_likelihood(initial_likelihood, updater)

tmle_params <- tmle_spec$make_params(tmle_task, targeted_likelihood)

(targeted_likelihood$updater$update_step(targeted_likelihood, tmle_task))

#Donvan estimator

weights <-1
get_estimates <- function(data, marker_var, outcome_var, thresholds, weights = rep(1, nrow(data))) {
  weights <- weights/sum(weights)
  data <- as.data.table(data)
  IC_list <- list()
  est_list <- list()
  cdf_list <- c()
  for(thresh in thresholds) {
    meets_thresh <- as.numeric(data[[marker_var]] >= thresh) 
    cdf <- weighted.mean(meets_thresh, weights)
    cdf_list <- c(cdf_list, cdf)
    EY <- weighted.mean(data[[outcome_var]] * meets_thresh, weights) / cdf
    thresh <- as.character(thresh)
    est_list[[thresh]] <- EY
    IC_list[[thresh]] <- meets_thresh/cdf * (data[[outcome_var]] - EY)
    
  }
  return(list(est = unlist(est_list),
             IC = do.call(cbind, IC_list), cdf = 1-cdf_list ))
}
out <- get_estimates(data, marker, outcome, thresholds)
est <- out$est
IC <- out$IC * weights
radius <- 1.96 * sqrt(resample::colVars(IC))/sqrt(nrow(data))
lower <- est - radius
upper <- est + radius
donovan_est <- data.table(lower, est, upper )

estimates <- tmle_params[[1]]$estimates(tmle_task)
# This should be very small
# Estimates of Psi
estimates$psi

# Estimates and confidence bounds
sumry <- summary_from_estimates(tmle_task, list(estimates))[,-c(1,2,3)]
lower <- sumry$lower
upper <- sumry$upper
est <- sumry$tmle_est
cutoffs <- thresholds
```

```{r, include = F}
cutoffs <- round(targeted_likelihood$factor_list$Y$learner$cutoffs,3)

plot_data <- data.frame(est = est, lower = lower, upper = upper, cutoffs = round(targeted_likelihood$factor_list$Y$learner$cutoffs,3))
plot_data2 <- donovan_est
plot_data2$cutoffs <- cutoffs
plot_data$grp <- "TMLE"
plot_data2$grp <- "Donovan"
plot_data <- rbind(plot_data, plot_data2)
library(ggplot2)

 g1 <- ggplot2::ggplot(data =  plot_data, aes_string("cutoffs", "est", group = "grp", col="grp", fill="grp"))+ scale_x_continuous(breaks = plot_data$cutoffs) +
                              geom_point(aes_string(x = "cutoffs", y = "est",group = "grp", col="grp", fill="grp"), legend=  F,   colour=alpha('red')) +
                              geom_point(aes_string(x = "cutoffs", y ="lower",group = "grp", col="grp", fill="grp"), legend=  F, alpha = 0) +
                              geom_point(aes_string(x = "cutoffs", y = "upper",group = "grp", col="grp", fill="grp"), legend=  F, alpha = 0) +
                              geom_smooth(se = F)+
                              geom_ribbon(aes(ymin = lower, ymax = upper), alpha= 0.2, color = NA) +
                              scale_x_continuous(breaks = plot_data$cutoffs)
 


```

```{r}
g1


```

