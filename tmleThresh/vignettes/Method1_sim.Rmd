---
title: "Method1_simulation"
output: html_document
---

```{r setup, include=FALSE}
#remotes::install_github("Larsvanderlaan/RStatisticalSoftware/tmleThresh")
devtools::document()
```


```{r}

remotes::install_github("Larsvanderlaan/tmle3", ref = "updating_updater")
```

```{r}
library(simcausal)
library(dplyr)


# variable: covariates W1,W2,W3. Immuresponse S. Disease Y. Sampling G.
## 3 cases for covariates: cov in {'non_covariate','non_confound','confound'}
## 4 models for P(Y|S,W): model in {'logit','probit','step','scale'}

# Examples:
## 1. non-covariate (model:'logit')
dat_logit<-simu(Risk=0.1,p0=1,p1=1,cov='non_covariate',model='logit') 
data1<-sim_thre(dat_logit,n = 1000,rndseed=1,p=c(0.01,0.03,0.05,0.07,0.09))
mean(data1$simmed_data$Y)

```

```{r}
library(simcausal)
D <- DAG.empty()
D <- D +
    node("W1", distr = "runif", min = -1, max = 1) +
    node("W2", distr = "rnorm", mean = 0) +
    node("W3full", distr = "rexp", rate = 1) +
  node("W3", distr = "rconst", const = min(W3full, 3)) +
  node("Sfull", distr = "rgamma", shape = abs(2 + W3full + W3full*W2 + W2*exp(W1) + cos(W2) + sin(W1))  ) +
    node("S", distr = "rconst", const = min(Sfull, 13)/13) +
  node("pY", distr = "rconst", const = 0.7*plogis(-3 + 4*S + 2*W1*(W2 >0) - 2*W2*(W1>0) + W3 * (S-0.1) - 2*sin(W2) - W3*cos(W1))) +
  node("Y", distr = "rbinom", size =1, prob = pY)


D <- set.DAG(D)
dat <- sim(D,n =1000)
dat
hist(dat$S)
table(dat$Y)
vs <- c(0, 0.1, 0.3, 0.5, 0.8)
for(v in vs) {
  print(sum(dat$Y * (dat$S>=v)) / sum(dat$S >=v))
}

D <- DAG.empty()
D <- D +
    node("W1", distr = "runif", min = -1, max = 1) +
    node("W2", distr = "rnorm", mean = 0) +
    node("W3full", distr = "rexp", rate = 1) +
  node("W3", distr = "rconst", const = min(W3full, 3)) +
  node("Sfull", distr = "rgamma", shape = abs(2.5)  ) +
    node("S", distr = "rconst", const = min(Sfull, 8)/8) +
  node("pY", distr = "rconst", const = 0.7*plogis(-2 + 3*S + 2*W1*(W2 >0) - 2*W2*(W1>0) + W3 * (S-0.1) - 2*sin(W2) - W3*cos(W1))) +
  node("Y", distr = "rbinom", size =1, prob = pY)


D <- set.DAG(D)
dat <- sim(D,n =1000)
dat
hist(dat$S)
table(dat$Y)
vs <- c(0, 0.1, 0.3, 0.5, 0.8)
for(v in vs) {
  print(sum(dat$Y * (dat$S>=v)) / sum(dat$S >=v))
}
```








```{r}
data <- dat
data
data$weights <- 1
library(tmle3)
#lrnr <- Lrnr_pooled_hazards$new(Lrnr_xgboost$new())
lrnr_Y <- Lrnr_xgboost$new()
lrnr_A <- Lrnr_xgboost$new()

tmle_spec <- tmle3_Spec_Threshold$new(method = "Psi_W")

learner_list <- list("A" = lrnr_A, "Y" =lrnr_Y)
node_list <- list("W" = c("W1", "W2"), "A" = "S", "Y" = "Y", weights = "weights")

start_time <- proc.time()

  tmle_task <- tmle_spec$make_tmle_task(data, node_list)
  task_time <- proc.time()

  initial_likelihood <- tmle_spec$make_initial_likelihood(tmle_task, learner_list)
  likelihood_time <- proc.time()

  updater <- tmle_spec$make_updater(maxit = 100, verbose = T)
  targeted_likelihood <- tmle_spec$make_targeted_likelihood(initial_likelihood, updater)

  tmle_params <- tmle_spec$make_params(tmle_task, targeted_likelihood)
  

suppressWarnings(targeted_likelihood$updater$update_step(targeted_likelihood, tmle_task))
```
0.5622048 0.5803265 0.5945332 0.6072527 0.6172666 0.6261565 0.6488544
 [8] 0.6725778 0.7239753 0.9393215
 
 0.5653488 0.5809175 0.5925751 0.6094160 0.6124955 0.6259612 0.6489949
 [8] 0.6636299 0.7057046 0.9216679
 
 
 0.5622048 0.5803265 0.5945332 0.6072527 0.6172666 0.6261565 0.6488544
 [8] 0.6725778 0.7239753 0.9393215

```{r}
estimates <- tmle_params[[1]]$estimates(tmle_task)
# This should be very small
colMeans(estimates$IC)
# Estimates of Psi
estimates$psi

# Estimates and confidence bounds
summary_from_estimates(tmle_task, list(estimates))
```





```{r}
sumry <- summary_from_estimates(tmle_task, list(estimates))
sumry
lower <- sumry$lower
upper <- sumry$upper
est <- sumry$tmle_est

plot_data <- data.frame(est = est, lower = lower, upper = upper, cutoffs = round(targeted_likelihood$factor_list$Y$learner$cutoffs,3))
cutoffs <- round(targeted_likelihood$factor_list$Y$learner$cutoffs,3)
library(ggplot2)

 ggplot2::ggplot(data =  plot_data, aes_string("cutoffs", "est"))+ scale_x_continuous(breaks = plot_data$cutoffs) +
                              geom_point(aes_string(x = "cutoffs", y = "est"), legend=  F,   colour=alpha('red')) +
                              geom_point(aes_string(x = "cutoffs", y ="lower"), legend=  F,  colour=alpha('red', 0.2)) +
                              geom_point(aes_string(x = "cutoffs", y = "upper"), legend=  F,  colour=alpha('red', 0.2)) +
                              geom_smooth(aes(colour = "red"),se = F)+
                              geom_ribbon(aes(ymin = lower, ymax = upper), alpha= 0.2, color = NA) +
                              scale_x_continuous(breaks = plot_data$cutoffs)
 
#Estimate thresholds and confidence interval via linear extrapolation

thresh <- function(p) {

   thresh_est <-approx(x = est, y = cutoffs, n = 100, xout = p)$y
   thresh_upper <-approx(x = lower, y = cutoffs, n = 100, xout = p)$y
   thresh_lower <-approx(x = upper, y = cutoffs, n = 100, xout = p)$y

  return(c(thresh_lower, thresh_est, thresh_upper))
}

thresh(0.7)
```

```{r}

```






```{r}
threshold_function = function(A) {as.vector(quantile(A, seq(0.1, 0.90, length.out = 10)))}
vs <- threshold_function(fit$tmle_task$get_tmle_node("A"))
for(v in vs) {
  num = sum(data$Y * (data$S >= v) * data$weights )
denom = sum((data$S >= v) * data$weights )
print(num/denom)

  
  
}


```


