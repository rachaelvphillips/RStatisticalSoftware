---
title: "Method1_simulation"
output: html_document
---

```{r setup, include=FALSE}
#remotes::install_github("Larsvanderlaan/RStatisticalSoftware/tmleThresh")
devtools::document()
```


```{r}

remotes::install_github("Larsvanderlaan/tmle3", ref = "updating_updater")
```



```{r}
library(simcausal)
D <- DAG.empty()
D <- D +
    node("W1", distr = "runif", min = -1, max = 1) +
    node("W2", distr = "rnorm", mean = 0) +
    node("W3full", distr = "rexp", rate = 1) +
  node("W3", distr = "rconst", const = min(W3full, 3)) +
  node("Sfull", distr = "rgamma", shape = abs(2 + W3full + W3full*W2 + W2*exp(W1) + cos(W2) + sin(W1))  ) +
    node("S", distr = "rconst", const = min(Sfull, 13)/13) +
  node("pY", distr = "rconst", const = 0.7*plogis(-3 + 4*S + 2*W1*(W2 >0) - 2*W2*(W1>0) + W3 * (S-0.1) - 2*sin(W2) - W3*cos(W1))) +
  node("Y", distr = "rbinom", size =1, prob = pY)


D <- set.DAG(D)
dat <- sim(D,n =1000)
dat
hist(dat$S)
table(dat$Y)
vs <- c(0, 0.1, 0.3, 0.5, 0.8)
for(v in vs) {
  print(sum(dat$Y * (dat$S>=v)) / sum(dat$S >=v))
}

D <- DAG.empty()
D <- D +
    node("W1", distr = "runif", min = -1, max = 1) +
    node("W2", distr = "rnorm", mean = 0) +
    node("W3full", distr = "rexp", rate = 1) +
  node("W3", distr = "rconst", const = min(W3full, 3)) +
  node("Sfull", distr = "rgamma", shape = abs(2.5)  ) +
    node("S", distr = "rconst", const = min(Sfull, 8)/8) +
  node("pY", distr = "rconst", const = 0.7*plogis(-2 + 3*S + 2*W1*(W2 >0) - 2*W2*(W1>0) + W3 * (S-0.1) - 2*sin(W2) - W3*cos(W1))) +
  node("Y", distr = "rbinom", size =1, prob = pY)


D <- set.DAG(D)
dat <- sim(D,n =1000)
dat
hist(dat$S)
table(dat$Y)
vs <- c(0, 0.1, 0.3, 0.5, 0.8)
for(v in vs) {
  print(sum(dat$Y * (dat$S>=v)) / sum(dat$S >=v))
}
```




```{r}
#Donvan estimator
get_estimates <- function(data, marker_var, outcome_var, thresholds, weights = rep(1, nrow(data))) {
  weights <- weights/sum(weights)
  data <- as.data.table(data)
  IC_list <- list()
  est_list <- list()
  for(thresh in thresholds) {
    meets_thresh <- as.numeric(data[[marker_var]] >= thresh) 
    cdf <- weighted.mean(meets_thresh, weights)
    EY <- weighted.mean(data[[outcome_var]] * meets_thresh, weights) / cdf
    thresh <- as.character(thresh)
    est_list[[thresh]] <- EY
    IC_list[[thresh]] <- meets_thresh/cdf * (data[[outcome_var]] - EY)
    
  }
  return(list(est = unlist(est_list),
             IC = do.call(cbind, IC_list) ))
}
thresholds <- quantile(data[["S"]], seq(0.1, 0.9, length.out = 10))
out <- get_estimates(data, "S", "Y", thresholds)
est <- out$est
IC <- out$IC
radius <- 1.96 * sqrt(resample::colVars(IC))/sqrt(nrow(data))
lower <- est - radius
upper <- est + radius
data.table(lower, est, upper )
```



```{r}
data <- dat
data
data$weights <- 1
library(tmle3)
#lrnr <- Lrnr_pooled_hazards$new(Lrnr_xgboost$new())
# To use machine learning
lrnr_Y <- Lrnr_xgboost$new()
lrnr_A <- Lrnr_xgboost$new()

tmle_spec <- tmle3_Spec_Threshold$new(method = "Psi_W")

learner_list <- list("A" = lrnr_A, "Y" =lrnr_Y)
node_list <- list("W" = c("W1", "W2"), "A" = "S", "Y" = "Y", weights = "weights")


tmle_task <- tmle_spec$make_tmle_task(data, node_list)

initial_likelihood <- tmle_spec$make_initial_likelihood(tmle_task, learner_list)

updater <- tmle_spec$make_updater(maxit = 100, verbose = T)
targeted_likelihood <- tmle_spec$make_targeted_likelihood(initial_likelihood, updater)

tmle_params <- tmle_spec$make_params(tmle_task, targeted_likelihood)
  

suppressWarnings(targeted_likelihood$updater$update_step(targeted_likelihood, tmle_task))
```


```{r}
estimates <- tmle_params[[1]]$estimates(tmle_task)
# This should be very small
colMeans(estimates$IC)
# Estimates of Psi
estimates$psi

# Estimates and confidence bounds
summary_from_estimates(tmle_task, list(estimates))
```





```{r}
sumry <- summary_from_estimates(tmle_task, list(estimates))
sumry
lower <- sumry$lower
upper <- sumry$upper
est <- sumry$tmle_est

plot_data <- data.frame(est = est, lower = lower, upper = upper, cutoffs = round(targeted_likelihood$factor_list$Y$learner$cutoffs,3))
cutoffs <- round(targeted_likelihood$factor_list$Y$learner$cutoffs,3)
library(ggplot2)

 ggplot2::ggplot(data =  plot_data, aes_string("cutoffs", "est"))+ scale_x_continuous(breaks = plot_data$cutoffs) +
                              geom_point(aes_string(x = "cutoffs", y = "est"), legend=  F,   colour=alpha('red')) +
                              geom_point(aes_string(x = "cutoffs", y ="lower"), legend=  F,  colour=alpha('red', 0.2)) +
                              geom_point(aes_string(x = "cutoffs", y = "upper"), legend=  F,  colour=alpha('red', 0.2)) +
                              geom_smooth(aes(colour = "red"),se = F)+
                              geom_ribbon(aes(ymin = lower, ymax = upper), alpha= 0.2, color = NA) +
                              scale_x_continuous(breaks = plot_data$cutoffs)
 
#Estimate thresholds and confidence interval via linear extrapolation

thresh <- function(p) {

   thresh_est <-approx(x = est, y = cutoffs, n = 100, xout = p)$y
   thresh_upper <-approx(x = lower, y = cutoffs, n = 100, xout = p)$y
   thresh_lower <-approx(x = upper, y = cutoffs, n = 100, xout = p)$y

  return(c(thresh_lower, thresh_est, thresh_upper))
}

thresh(0.7)
```






```{r}
threshold_function = function(A) {as.vector(quantile(A, seq(0.1, 0.90, length.out = 10)))}
vs <- threshold_function(fit$tmle_task$get_tmle_node("A"))
for(v in vs) {
  num = sum(data$Y * (data$S >= v) * data$weights )
denom = sum((data$S >= v) * data$weights )
print(num/denom)

  
  
}


```


