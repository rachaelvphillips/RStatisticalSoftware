---
title: "SurvivalSimulation"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r}

remotes::install_github("osofr/simcausal")
library(simcausal)

```


The package sincausal allows one to simulate from user defined structural equation models. The main idea is to define a set of nodes which are causally ordered, each with their own distribution. Their distribution might depend on the values of past nodes. Therefore, the data must be simulated sequentially, following the causal ordering.

For an introduction to the package, see the github:
https://github.com/osofr/simcausal

# Sample survival data (W, A, Delta, Ttilde) from scratch

```{r}
#Lets draw data from a parametric distribution using the package simcausal (github)
# We will assume that this is the real data from our study


D <- DAG.empty()
D <- D +
  node("W", distr = "runif", min =1, max = 25) +
  node("A", distr = "rbinom", size = 1, prob = 0.2 + W/50 ) +
  node("T", distr = "rexp", rate = 0.05*(1 - 0.5*A + 0.3*W) )+
  node("C", distr = "rexp",  rate = 0.03*(1 - 0.4*A + 0.2*W)) +
  node("Delta", distr = "rconst", const = as.numeric(T<=C)  ) +
  node("Ttilde", distr = "rconst", const = round(min(T,C,10) +1  ))
setD <- set.DAG(D)
# Lets draw 200 samples
dat <- sim(setD, n = 200)
head(dat)
```

```{r}
#Lets assume we are outcome blind.
# Thus, we are only given the outcome blind data (W, A)
baseline_data <- dat[, c("W", "A")]
#We can learn P(A |W) from the given data. For simplicty, lets use logistic regression
fit <- glm("A ~ W", family = binomial(), data = baseline_data)
#Lets store P(A=1|W) as a function of w
PA1_W <- function(w){
  as.vector(predict(fit, newdata = list(W = w), type = "response"))
}
PA1_W(0.5)

```

# Simulate survival data from a known (empirical distribution) of W and estimated conditional density of binary A and parametric forms for censoring/survival

```{r}

# Our goal is to sample (W, A, Delta, Ttilde) survival data structure where W and A are sampled from distributions estimated from the data.
# Lets estimate P(W) with the empirical distribution so that sampling from it is trivial
#Next, we assume that A is binary so that once we have drawn W = w, we can draw A using rbinom(1, size = 1, prob = P(A=1|W = w) ). 


n = 50
Wsamp = baseline_data$W
#This function samples from an empirical sample
remp <- function(n){
  emp_sample <- Wsamp
  sample(emp_sample, n, replace = T)
}

# Should be the function P(A = 1 |W).
prob_map <- PA1_W
#Make sure this map is vectorized if needed
prob_map <- Vectorize(prob_map)
#This function samples from A conditional on some value (i.e. W in this case) using our estimated conditional distribution of A
rTrtment <- function(n, value, prob){
  rbinom(n, size = 1, prob= prob(value))
}


#Now, we would like to draw survival data. Specifically, we want to draw a time of death T and time of censoring C which would then imply (Ttilde, Delta).
#For this, we need to specify the conditional distributions P(T <= t| A, W) and 
# P(C <= t| A, W). 
# We can parametrically model these conditional distributions (as shown below) or we can estimate these distributions from data. In the case where we are outcome blind, we might still have access to real world survival data from a different study. We can use the estimated conditional distributions from this other data in our simulation. 


#First, let us use a parametric form for these conditional distributions of survival and censoring. We will assume they are drawn from a Weibull distribution
#where the shape and scale parameters depend on A and W
D <- DAG.empty()
D <- D +
  # This node represents a sample of W's from our empirical distribution
  node("W", distr = "remp") + 
    # This node represents a sample of A's from our conditional distribution of A given the previously drawn W 
  node("A", distr = "rTrtment", value = W, prob =  prob_map) +
   # This node represents a drawn survival time from a parametric form of the conditional survival distribution evaluated at the previously drawn W and A
  node("T", distr = "rweibull", shape = W + 5*A + W*A, scale = W + 5*A - 0.2*A*W) +
  # Same as above but for censoring
    node("C", distr = "rweibull", shape = W + 5*A + W*A, scale = W + 5*A - 0.205*A*W)+
  # For simplicity, lets make the times discrete by rounding up
  node("Tdiscrete", distr = "rconst", const = ceiling(T)) +
    node("Cdiscrete", distr = "rconst", const = ceiling(C)) +
  #Now we extract Ttilde and Delta
  node("Ttilde", distr = "rconst", const = min(Tdiscrete, Cdiscrete)) +
  node("Delta", distr = "rconst", const = as.numeric(Ttilde  == Tdiscrete) )


setD <- set.DAG(D)
#Now we simulate the data by drawing n = 200 samples
dat <- sim(setD, n = 200)
# only grab ID, W's, A, T.tilde, Delta
head(dat)
#Now we can extract our simulated observed data
observed_data <- dat[, c("W", "A", "Ttilde", "Delta")]
head(observed_data)
```


# Simulate data from an estimated survival distribution

Now, lets assume we have an estimated conditional survival function. 

```{r}
#Lets assume our estimated survival is exactly the pweibull distribution we previously used for simulation for simplicty.
est_surv <- function(t, A, W){
  #This could be any estimated survival function (e.g. survival from cox fit)
  1 - pweibull(t, shape = W + 5*A + W*A, scale = W + 5*A - 0.2*A*W)
}
# We need the CDF for sampling
est_CDF <- function(t, A ,W){
  return(1 - est_surv(t, A, W))
}
library(stats)
#This function takes a (absolutely) continuous CDF and returns the inverse
#Note if our survival data is discrete time then we will need to drawn in a different way using the hazard function representation.
# Function to numerically compute inverse:
est_cdf_inverse <- function(p, A ,W){
  f_zero <- function(t){
    est_CDF(t, A, W) - p
  }
  grid_endpoints <- c(0, 30)
  root <- stats::uniroot(f_zero, interval = grid_endpoints)$root
  return(root)
} 
#We need this to be vectorized
est_cdf_inverse <- Vectorize(est_cdf_inverse)
# True inverse using qweibull
true_inverse <- function(p , A , W){
  qweibull(p, shape = W + 5*A + W*A, scale = W + 5*A - 0.2*A*W)
}
#They are the same
true_inverse(0.5, 1, 10)
est_cdf_inverse(0.5, 1, 10)
```


````{r}

#Inverse transform (CDF) sampling
n_samples <- 100
A = 1
W = 10
#draws n samples from condiitonal dist given A and W
do_sample <- function(n, A, W){
  #Draw uniform random variables and feed into inverse CDF
  est_cdf_inverse(runif(n), A = A, W= W)
}

#simulated data summary stat
samples <- do_sample(n_samples, A , W)
mean(samples)
sd(samples)

# simulate from rweibull
samples <- rweibull(n_samples, shape = W + 5*A + W*A, scale = W + 5*A - 0.2*A*W)

mean(samples)
sd(samples)

#As expected, the two samples have similar means and variances
#As they are sampling from the same distribution.
```


```{r}
#Now, lets use our "do_sample" function to simulate the W A T C data.
# We will use the parametric form of the censoring for simplicity. Though the previous work can be applied to C equally well.
# The only changes are in the "T" node
D <- DAG.empty()
D <- D +
  # This node represents a sample of W's from our empirical distribution
  node("W", distr = "remp") + 
    # This node represents a sample of A's from our conditional distribution of A given the previously drawn W 
  node("A", distr = "rTrtment", value = W, prob =  prob_map) +
   # This node represents a drawn survival time from a parametric form of the conditional survival distribution evaluated at the previously drawn W and A
  node("T", distr = "do_sample", A = A, W = W) +
  # Same as above but for censoring
    node("C", distr = "rweibull", shape = W + 5*A + W*A, scale = W + 5*A - max(0.205*A*W, 0))+
  # For simplicity, lets make the times discrete by rounding up
  node("Tdiscrete", distr = "rconst", const = ceiling(T)) +
    node("Cdiscrete", distr = "rconst", const = ceiling(C)) +
  #Now we extract Ttilde and Delta
  node("Ttilde", distr = "rconst", const = min(Tdiscrete, Cdiscrete)) +
  node("Delta", distr = "rconst", const = as.numeric(Ttilde  == Tdiscrete) )


setD <- set.DAG(D)
#Now we simulate the data by drawing n = 200 samples
dat <- sim(setD, n = 200)
head(dat)
#Now we can extract our simulated observed data
observed_data <- dat[, c("W", "A", "Ttilde", "Delta")]
head(observed_data)

# We now have our survival data using a custom conditional survvial fit.

```



# Simulation of survival times via hazard estimates for censoring and survival

```{r}
time_points = 1:10
hazard_f <- function(t, A, W){
  #some hazard of t, A and W
  plogis(-2 - 0.2*A + W/100)
}
hazard_f <- Vectorize(hazard_f)

hazard_g <- function(t, A, W){
  #some hazard of t, A and W
  plogis(-6)
}
hazard_g <- Vectorize(hazard_f)
D <- DAG.empty()
D <- D +
  node("W", distr = "runif", min =1, max = 25) +
  node("A", distr = "rbinom", size = 1, prob = 0.2 + W/50 ) +
  node("dNt", t = time_points,  distr = "rbern",  p = hazard_f(t, A, W) , EFU  = T)+  node("dAt", t = time_points,  distr = "rbern",  p = hazard_g(t, A, W) , EFU  = T)

# Lets draw 200 samples
setD <- set.DAG(D, vecfun = c("hazard_f", "hazard_g"))
dat <- sim(setD, n = 200)
head(dat)

dNt_data <- dat[, grep( "dNt", colnames(dat))]
survival_times <- apply(dNt_data,1, function(row){
  result = time_points[!is.na(row) & row ==1]
  if(length(result)==0){
    return(NA)
  }
  return(result)
})

dAt_data <- dat[, grep( "dAt", colnames(dat))]
censor_times <- apply(dAt_data,1, function(row){
  result = time_points[!is.na(row) & row ==1]
  if(length(result)==0){
    return(max(time_points))
  }
  return(result)
})

#NA means censoring happened first
head(data.frame(cbind(survival_times, censor_times)))
```

# Estimating hazards

```{r}

library(sl3)
library(simcausal)
D <- DAG.empty()
D <- D +
  node("W", distr = "runif", min =1, max = 25) +
  node("A", distr = "rbinom", size = 1, prob = 0.2 + W/50 ) +
  node("T", distr = "rexp", rate = 0.05*(1 - 0.5*A + 0.3*W) )+
  node("C", distr = "rexp",  rate = 0.03*(1 - 0.4*A + 0.2*W)) +
  node("Delta", distr = "rconst", const = as.numeric(T<=C)  ) +
  node("Ttilde", distr = "rconst", const = round(min(T,C,10) +1  ))
setD <- set.DAG(D)
# Lets draw 200 samples
dat <- sim(setD, n = 200)
head(dat)


```




```{r}
data <- dat[,c("ID", "W", "A", "Delta", "Ttilde")]
head(data)
time_range <- range(data$Ttilde)
times <- seq.int(min(time_range), max(time_range))
list_of_data <- lapply(times, function(t) {
  #copy data
  data <- data
  data$t <- t
  # Outcome is 1 if and only if person dies at time t
  data$dN <- as.numeric(data$Ttilde == t & data$Delta == 1)
  data$dC <- as.numeric(data$Ttilde == t & data$Delta == 0)
  return(data)
})


data_pooled <- do.call(rbind, list_of_data)
head(data_pooled)

# Estimate survival hazard
covariates <- c("W", "A", "t")
outcome <-  "dN"
# Create sl3 regression task. The task essentially keeps track of the information needed for regression.
# E.g. data, covariate names, outcome names, time and ids for cross validation.
task_survival <- sl3_Task$new(data_pooled, covariates = covariates, outcome = outcome, outcome_type = "binomial", time = "t", id = "ID")

covariates <- c("W", "A", "t")
outcome <-  "dC"
task_censoring <- sl3_Task$new(data_pooled, covariates = covariates, outcome = outcome, outcome_type = "binomial", time = "t", id = "ID")
head(task_survival$data)
```

```{r}
# For estimation, we only keep rows corresponding to times where the person is still alive/not censored
keep_surv <-  data_pooled$t <= data_pooled$Ttilde
data_pooled_training_surv <- data_pooled[keep_surv,]
covariates <- c("W", "A", "t")
outcome <-  "dN"
task_survival_training <- sl3_Task$new(data_pooled_training_surv, covariates = covariates, outcome = outcome, outcome_type = "binomial", time = "t", id = "ID")
# or cleaner
task_survival_training <- task_survival[keep_surv]
head(task_survival_training$data)
tail(task_survival_training$data)
dim(task_survival_training$data)



# For estimation, we only keep rows corresponding to times where the person is still alive/not censored.
# We usually assume that we observe whether someone is alive before they get censored Therefore, if someone has died at time t, they are no longer at risk of being censored at time t. As a result, we also remove all rows where people have died. Thus, the risk sets of censoring and survival are a bit different
### This done so we have a time ordering in the likelihood. Only needed for discrete time.
keep_censoring <-  data_pooled$t <= data_pooled$Ttilde & (data_pooled$dN != 1)
task_censoring_training <- task_censoring[keep_censoring]
head(task_censoring_training$data)
tail(task_censoring_training$data)
dim(task_censoring_training$data)
```

```{r}
# We now have 4 tasks, 2 for estimation and 2 for prediction.
# Removing rows is needed so that the estimation is correct.
# However we want the hazard predictions at all time points for all people, so dont remove rows.
task_survival
task_survival_training
task_censoring
task_censoring_training
```


```{r}
#survival training
# Now, we need a binomial learner.
lrnr_glm <- make_learner(Lrnr_glm, family = binomial())
# train using training task
lrnr_glm_trained <- lrnr_glm$train(task_survival_training)
# predict using full task
hazard_survival_preds <- lrnr_glm_trained$predict(task_survival)
long_preds <- (data.frame(id = task_survival$id, t = task_survival$time, preds = hazard_survival_preds))
print(head(long_preds))
wide_preds <- reshape(long_preds, timevar = "t", idvar = "id", direction = "wide")
colnames(wide_preds) <- c("id", times)
head(wide_preds)

# It looks like the hazard probability increases with time.
survival_preds <- data.frame(cbind(wide_preds[,1], t(apply(1 -wide_preds[,-1],1, cumprod))))
colnames(survival_preds) <- c("id", times)
head(survival_preds)
```

```{r}
#censoring training
# Now, we need a binomial learner.
lrnr_glm <- make_learner(Lrnr_glm)
# train using training task
lrnr_glm_trained <- lrnr_glm$train(task_censoring_training)
# predict using full task
hazard_censoring_preds <- lrnr_glm_trained$predict(task_censoring)
long_preds <- (data.frame(id = task_censoring$id, t = task_censoring$time, preds = hazard_censoring_preds))
print(head(long_preds))
wide_preds <- reshape(long_preds, timevar = "t", idvar = "id", direction = "wide")
colnames(wide_preds) <- c("id", times)
head(wide_preds)

# It looks like the hazard probability increases with time.
censoring_survival_preds <- data.frame(cbind(wide_preds[,1], t(apply(1 -wide_preds[,-1],1, cumprod))))
colnames(censoring_survival_preds) <- c("id", times)
head(censoring_survival_preds)
```


```{r}
#survival
#Lets use xgboost now with depth 3
lrnr_xgboost <- make_learner(Lrnr_xgboost, max.depth = 3)
lrnr_xgboost_trained <- lrnr_xgboost$train(task_survival_training)
hazard_survival_preds <- lrnr_xgboost_trained$predict(task_survival)
long_preds <- (data.frame(id = task_survival$id, t = task_survival$time, preds = hazard_survival_preds))
print(head(long_preds))
wide_preds <- reshape(long_preds, timevar = "t", idvar = "id", direction = "wide")
colnames(wide_preds) <- c("id", times)
head(wide_preds)
survival_preds <- data.frame(cbind(wide_preds[,1], t(apply(1 -wide_preds[,-1],1, cumprod))))
colnames(survival_preds) <- c("id", times)
head(survival_preds)
```

```{r}

# And hal9001 (highly adaptive lasso)


lrnr_hal9001<- make_learner(Lrnr_hal9001, max_degree = 2)
lrnr_hal9001_trained <- lrnr_hal9001$train(task_survival_training)
hazard_survival_preds <- lrnr_hal9001_trained$predict(task_survival)
long_preds <- (data.frame(id = task_survival$id, t = task_survival$time, preds = hazard_survival_preds))
print(head(long_preds))
wide_preds <- reshape(long_preds, timevar = "t", idvar = "id", direction = "wide")
colnames(wide_preds) <- c("id", times)
head(wide_preds)
survival_preds <- data.frame(cbind(wide_preds[,1], t(apply(1 -wide_preds[,-1],1, cumprod))))
colnames(survival_preds) <- c("id", times)
head(survival_preds)
```


```{r}
# We can also do the estimation without sl3
covariates <- c("W", "A", "t")
outcome <-  "dN"
id <- data_pooled$ID

data_pooled_glm <- data_pooled[, c(covariates, outcome)]
data_pooled_training_surv_glm <- data_pooled_training_surv[, c(covariates, outcome)]
head(data_pooled)
head(data_pooled_training_surv)

# need to add intercpt manually
x_train = cbind(rep(1, nrow(data_pooled_training_surv_glm)), data_pooled_training_surv_glm[, covariates])
colnames(x_train) <- c("intercept", covariates)
y_train = data_pooled_training_surv_glm[, outcome]
fit <- glm.fit(x_train, y_train, family = binomial())
x_pred = cbind(rep(1, nrow(data_pooled_glm)), data_pooled_glm[, covariates])
coefs <- fit$coefficients
coefs
preds <- data.frame(t = data_pooled_glm$t, id =id, preds = plogis(as.matrix(x_pred) %*% coefs ))
wide_preds <- reshape(preds, idvar = "id", timevar = "t", direction = "wide")
head(wide_preds)
survival_preds <- data.frame(cbind(wide_preds[,1], t(apply(1 -wide_preds[,-1],1, cumprod))))
colnames(survival_preds) <- c("id", times)
head(survival_preds)
```
