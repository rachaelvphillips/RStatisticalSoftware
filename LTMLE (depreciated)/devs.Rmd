
```{r, include=F}
#remotes::install_github("Larsvanderlaan/tmle3", ref = "devel")


devtools::document()
```
```{r}
setkey(data, id ,t)
key(data[, c("t", "id"), with = F])
```
```{r}

rbinom(11, 1, c(rep(0,5), rep(1,5)))
```
```{r}
library(simcausal)
n = 50
Wsamp = runif(n, 1, 25)
remp <- function(n, emp_sample){
  sample(emp_sample, n, replace = T)
}

# Should be the function P(A = 1 |W).
#For simplicty, we will just assume P(A=1 | W) = logit(W)
prob_map <- function(w) {plogis(w%%1)}
prob_map <- Vectorize(prob_map)
rTrtment <- function(n, value, prob){
  rbinom(n, size = 1, prob= prob(value))
}



D <- DAG.empty()
D <- D +
  node("W", distr = "remp", emp_sample = Wsamp) + 
  node("A", distr = "rTrtment", value = W, prob =  prob_map) +
  node("T", distr = "rweibull", shape = W + 5*A + W*A, scale = W + 5*A - 0.2*A*W) +
    node("C", distr = "rweibull", shape = W + 5*A + W*A, scale = W + 5*A - 0.205*A*W)+
  node("Tdiscrete", distr = "rconst", const = ceiling(T)) +
    node("Cdiscrete", distr = "rconst", const = ceiling(C)) +
  node("Ttilde", distr = "rconst", const = min(Tdiscrete, Cdiscrete)) +
  node("Delta", distr = "rconst", const = as.numeric(Ttilde  == Tdiscrete) )


setD <- set.DAG(D)
dat <- sim(setD, n = 100)
# only grab ID, W's, A, T.tilde, Delta
dat

```

```{r}
library(data.table)

# Generate simple causal model
library(simcausal)

D <- DAG.empty()
D <- D +
  node("W", distr = "runif", min =-1, max = 1) +
  node("A", distr = "rbinom", size = 1, prob = 0.5 + W/4 ) +
  node("T", distr = "rexp", rate = 0.05*(1 + 0.5*A - 0.3*W) )+
  node("C", distr = "rexp",  rate = 0.03*(1 + 0.4*A - 0.2*W)) +
  node("Delta", distr = "rconst", const = as.numeric(T<=C)  ) +
  node("Ttilde", distr = "rconst", const = round(min(T,C,10) +1  ))
setD <- set.DAG(D)
dat <- sim(setD, n = 1e3)
# only grab ID, W's, A, T.tilde, Delta
Lname <- grep("W", colnames(dat), value = TRUE)
Aname <- grep("A", colnames(dat), value = TRUE)
df <- dat[, c("ID", Lname, Aname, "T", "C", "Delta", "Ttilde")]

data <- df[,c( "W", "A", "Ttilde", "Delta")]
data$id = df$ID

data$t <- data$Ttilde 
data = data.table(data)
data$processA = as.numeric(data$Delta ==0)
data$processN = as.numeric(data$Delta ==1)
data_baseline <- data
data_baseline$t =0
data_baseline$processA = 0
data_baseline$processN = 0
data_baseline$Ttilde = NA
data_baseline$Delta = NA
data = rbind(data_baseline, data)
data$Ttilde = NULL
data$Delta = NULL
data$at_risk = 1 - (data$processA + data$processN)
data[which(data$processN==1), "processA"] = 0
npsem = list(define_lnode("W", c("W"), time =0), 
     define_lnode("A", c("A"), c("W"), time =0, summary_functions = make_summary_measure_baseline(c("W"))),
     
     define_lnode("processA", c("processA"),  c("A","W"), time = "pooled",times_to_pool = seq(1,max(data$t)), summary_functions = make_summary_measure_baseline(c("A", "W")),
                  at_risk_map = make_summary_measure_last_value("at_risk", strict_past  = T), missing_row_implies_not_at_risk = F),
     
define_lnode("processN", c("processN"), c("A","W"),time = "pooled",times_to_pool = seq(1,max(data$t)), summary_functions = make_summary_measure_baseline(c("A", "W")),
                  at_risk_map = make_summary_measure_last_value("at_risk", strict_past = T), missing_row_implies_not_at_risk = F))
# Delta is the counting process which is 1 at death
```


```{r}
remotes::install_github("Larsvanderlaan/RStatisticalSoftware/Halgene")
```


```{r}
task = ltmle3_Task$new(data, npsem )

task$get_regression_task("processN", expand = F)$data
```






```{r}
tlik = Targeted_Likelihood_pooled$new(lik, updater = tmle3_Update$new(one_dimensional = T,constrain_step=T, delta_epsilon = 0.1, maxit = 100))
param <- Param_survival$new(tlik, intervention_list = list("A" = LF_static$new("A", value = 1)), outcome_node = "processN")
param$get_EIC_var(task)
```


```{r}

param <- Param_survival$new(tlik, intervention_list = list(), outcome_node = "processN")

#subdata<- tlik$updater$generate_submodel_data(tlik, task, update_node_key = "processN")



```




```{r}

tlik$updater$update(tlik, task)
```

```{r}
tlik$get_likelihood(task, "processN")
lik$get_likelihood(task, "processN")
```


```{r}
lik$get_likelihood(task, "processN")
tlik$get_likelihood(task, "processN")
```


```{r}
 eps = tlik$updater$fit_submodel(subdata)
coef(eps)
preds<- predict(eps, type = "response")
obs <- subdata$observed[[1]]
mean(subdata$loss(preds,obs))
mean(subdata$loss(subdata$initial[[1]],obs))
data.table(subdata$H[[1]])
data.table(cbind(preds, subdata$initial[[1]],subdata$observed[[1]]))
data.table(cbind(task$get_regression_task("processN", drop_censored = T)$data[,processN],preds, subdata$observed[[1]]))
```

```{r}
sub <- tlik$updater$fit_submodel(subdata)
dim(sub$H)
H <- unlist(sub$H, use.names = F)

obs <- sub$observed
init <- sub$initial
 data.table(sub$H)
data.table(obs)
data.table(init)

 fit = glm(observed ~ H - 1, sub,
                                offset = qlogis(init),
                                family = binomial(),
                                start = rep(0, 1))
coef(fit)

```



```{r}

 # Generate simple causal model
 library(simcausal)
n=10000
 D <- DAG.empty()
 D <- D +
   node("W", distr = "rbinom", size = 1, prob = 0.5 )  +
   node("A", distr = "rbinom", size = 1, prob = 0.5 + W/4 ) +
     node("L1", distr = "rbinom", size = 1, prob = 0.5 + A/4 ) + 
     node("A1", distr = "rbinom", size = 1, prob = 0.5 + L1/4 ) +
     node("L2", distr = "rbinom", size = 1, prob = 0.5 + A1/4 ) + 
     node("A2", distr = "rbinom", size = 1, prob = 0.5 + L2/4 ) +
     node("L3", distr = "rbinom", size = 1, prob = 0.5 + A2/4 ) + 
     node("A3", distr = "rbinom", size = 1, prob = 0.5 + L3/4 ) +
     node("L4", distr = "rbinom", size = 1, prob = 0.5 + A3/4 ) + 
     node("A4", distr = "rbinom", size = 1, prob = 0.5 + L4/4 ) +
   node("Y", distr = "rbinom", size = 1, prob = 0.5 + A4/4 )
  
 setD <- set.DAG(D)
 dat <- sim(setD, n = n)
dat
L=unlist(dat[,stringr::str_detect(colnames(dat), "[WL]")], use.names = F)
A=unlist(dat[,stringr::str_detect(colnames(dat), "[A]")], use.names = F)
Y = dat$Y
dat
```

```{r}
dat=data.table(id = rep(1:n, 5), L = L, A = A, Y = Y, 
           t = c(rep(0, n), rep(1, n),rep(2, n),rep(3, n),rep(4, n)))
```

```{r}
lrnr = Lrnr_glm$new()
factor_list = list(LF_emp$new("W"),
LF_fit_pooled$new("A", lrnr),
LF_fit_pooled$new(c("A1", "A2", "A3", "A4"),lrnr),
LF_fit_pooled$new(c("L1", "L2", "L3", "L4"),lrnr),
LF_fit_pooled$new("Y",lrnr))
npsem = list(define_lnode("W", "L", c(), time = 0),
     define_lnode("A", "A", c("W"), time = 0),
     define_lnode("L1", "L", c("A"), time = 1),
      define_lnode("L2", "L", c("A1"), time = 2),
      define_lnode("L3", "L", c("A2"), time = 3),
      define_lnode("L4", "L", c("A3"), time = 4),
      define_lnode("A1", "A", c("L1"), time = 1),
      define_lnode("A2", "A", c("L2"), time = 2),
      define_lnode("A3", "A", c("L3"), time = 3),
      define_lnode("A4", "A", c("L4"), time = 4),
     define_lnode("Y", "Y", c("A4"), time = 4)
     
     
     
     )
lik = Likelihood_pooled$new(factor_list)
```

```{r}
matrix(list(as.vector(c(1,2)), as.vector(c(2,3))))
```
```{r}
task = ltmle3_Task$new(dat, npsem, extra_summary_measure_columns = NULL, thin =F)
t=proc.time()
f=task$get_regression_task("Y")
#lik_tr = lik$train(task)
#lik_tr$get_likelihoods(task)
t-proc.time()
sr = Sampler$new(lik_tr, c("W", "A", "L1", "A1",  "L2", "A2" , "L3", "A3" , "L4", "A4", "Y"), "W")

```













